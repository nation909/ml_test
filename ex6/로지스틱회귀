로지스틱회귀
참, 거짓으로만 판단해야하는 경우 사용
딥러닝에서는 주로 전달받은 정보를 가지고 참, 거짓을 판단해서 다음단계로 넘기는 방식으로 사용함

EX.
공부한시간: [2, 4, 6, 8, 10, 12, 14]
합격 여부: ['불합격', '불합격', '불합격', '합격', '합격', '합격', '합격']

로지스틱회귀도 선형회귀와 마찬가지로 0(불합격), 1(합격)으로 선을그림
- S자 형태의 선으로 그려짐

시그모이드 함수
- S자 형태로 그래프가 그려지는 함수

시그모이드함수 방정식
y = 1 / (1 + e^(-ax + b))

e: 자연상수라고 불리는 무리수(값은 2.71828...)
- 수학에서 파이처럼 상수로 고정된 값으로 따로 구하는 값은 아님
a: 그래프의 경사도를 결정(a값이 크면 경사가크고, a값이 작으면 경사가 작음)
b: 그래프의 좌,우 이동을 의미(b값이 작고 큼에따라 그래프가 이동함)

실제로 구해야할 값은 a, x, b를 구하면 됌

로그함수
- 시그모이드함수의 특징은 y값이 0~1사이의 값임
- EX. 실제값이 0일경우 예측값이 1로 가면 갈수록 오차가 커지고
-  실제값이1일때 예측값이 0으로 가면 갈수록 오차가 커짐
- 이를 공식으로 만들수 있는 함수가 로그함수임

y의 실제값이 0일때 -log(1-h)를 사용
y의 실제값이 1일때 -log h를 사용

# 오차 = -평균(y * log h + (1 - y) * log(1 - h))
               ----A-----  ----------B---------
A: 실제값이 0일때
B: 실제값이 1일때

h: 시그모이드함수에서나온 y값 인듯?


내용 정리
입력값을 통해 출력값을 구하는 함수 식
y = (a1 * ax) + (a2 * x2) + b
x1: 공부한시간
x2: 과외받은횟수
a1: x1의 가중치(경사도)
a2: x2의 가중치(경사도)
b: 그래프의 좌/우 이동(b값이 작고 큼에따라 그래프가 이동함)



다중로지스틱회귀 프로세스
# 공부한시간, 과외받은횟수, 실제합격/불합격여부 데이터로 합격/불합격 예측하기

x_data = [공부한시간, 과외받은횟수] 담기
y_data = 합격/불합격 담기

a = 기울기를 랜덤으로 담음
b = 바이어스값을 랜덤으로 담음

그래프를 그리기위해 시그모이드함수를 이용해서 y값을 구함
y = 1 / (1 + e^(-ax + b)) 단일일경우
다중로지스틱회귀이기 때문에 다중으로 사용 (matmul함수로 행렬을 곱해주고 +b를 더함 sigmoid함수로
y = tf.sigmoid(tf.matmul(X, a) + b)

로그함수로 오차를 구함
오차 = -평균(y * log(h) + (1 - y) * log(1 - h))
h: 시그모이드함수로 구한 y값


경사하강법으로 오차를 최소화된 값을 구함

텐서플로우로 학습시킴
