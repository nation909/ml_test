# 오차 역전파
가중치를 몰라도 문제를 해결하기 위한 방법
경사하강법의 확장개념으로 다중퍼셉트론에서 오차가 최소가 되도록 하는 개념
임의의 가중치를 주고 결과를 계산하고 계산결과의 오차를 구한다
경사하강법을 이용해 바로앞의 가중치를 오차가 적은방향으로 업데이트를 반복한다

W(t + 1) = Wt - partial(오차) / partial(W)
Wt: 현재 가중치
W(t + 1): 새로운 가중치
오차: 가중치에대한 기울기를 뺀 값
partial: 편미분은 여러가지변수가 식에 있을때 모든변수를 미분하는게 아니라 원하는 한가지변수(ex. 가중치)만 미분하고
그외에는 상수로 취급하라는 뜻

입력된 실제값과 다층 퍼셉트론의 계산결과를 비교하여 가중치를 수정해가는 방법
프로세스
1. 환경변수지정: 입력값, 결과값, 학습률, 활성화함수, 가중치 선언필요
2. 신경망 실행: 초기값을 입력하여 활성화함수와 가중치를 거쳐 결과값이 나오게 실행
3. 결과를 실제값과 비교: 결과값과 실제값을 비교하여 오차를 측정함
4. 역전파 실행: 출력층과 은닉층의 가중치를 수정
5. 결과 출력

기울기 소실문제와 활성화 함수
층이 늘어나면서 기울기가 중간에 0이 되어버리는 기울기 소실문제가 발생
시그모이드함수를 활성화함수로 사용하는데
미분을 하면 최대치가 0.3인데 1보다 작아서 계속 곱하다보면 0에 가까워지다가
층을 거쳐갈수록 기울기가 사라져 가중치를 수정하기가 어려워짐

# 시그모이드 대체 함수
하이퍼볼릭 탄젠트: 미분한 값의 범위를 확장
- 시그모이드 함수범위를 -1에서 1로 확장한 개념
- 여전히 1보다 작은 값이 존재하므로 기울기소실문제는 사라지지 않음

렐루: 시그모이드 대안으로 떠오르며 가장많이 사용하는 활성화 함수
- x가 0보다 작을때는 모든 값을 0으로 처리하고, 0보다 큰 값은 x를 그대로 사용하는 방법
- 단순해보이지만, 여러 은닉층을 거치며 곱해지더라도 맨 처음층까지 사라지지 않음

소프트플러스: 렐루의 0이 되는 순간을 완화한 렐루를 변형한 함수도 개발중

속도와 정확도 문제를 해결하는 고급 경사 하강법
- 기존 경사 하강법은 정확하게 가중치를 찾아가지만 한번 업데이트할때마다 전체 데이터를 미분해야해서 계산량이 매우 많음
- 이러한 점을 보완한 방법이 고급 경사 하강법

# 고급 경사 하강법 방법
1. 확률적 경사 하강법(SGD)
- 전체데이터에서 랜덤하게 추출한 일부데이터를 사용하여 계산량 단축
- 장점: 속도 개선

2. 모멘텀
- 단어는 관성, 탄력, 가속도라는 뜻으로 경사하강법에 탄력을 더해줌
- 경사하강법처럼 매번 기울기를 구하지만 오차를 수정하기전 바로 앞 수정값과 방향(+, -)을 참고하여
- 같은 방향으로 일정한 비율만 수정되게 하는 방법
- 따라서 수정방향이 양수(+)방향으로 한번, 음수(-)방향으로 한번 지그재그로 일어나는 현상이 줄고
- 이전 이동 값을 고려해서 일정 비율만큼만 다음값으로 결정함
- 장점: 정확도 개선

3. 네스테로프 모멘텀(NAG)
- 모멘텀이 이동시킬 방향으로 미리 이동해서 그레이디언트를 계산
- 장점: 정확도 개선, 불필요한 이동 줄임

4. 아다그라드(Adagrad)
- 변수의 업데이트가 잦으면 학습률을 적게하여 이동 보폭을 조절
- 보폭크기 개선

5. 알엠에스프롬(RMSProp)
- 아다그라드의 보폭 민감도를 보완한 방법
- 보폭크기 개선

6. 아담(Adam)
- 모멘텀과 알엠에스프롬 방법을 합친 방법으로 현재 가장많이 사용되는 고급경사하강법
- 정확도와 보폭크기 개선